/*
 * software-packet-distributor
 * SPDX-License-Identifier: BSD-3-Clause
 * Copyright (c) 2026 Mike Chang
 * Author: Mike Chang <mikechang.engr@gmail.com>
 */
#include "core_distributor.h"
#include "globals.h"
#include "hash.h"
#include "fat.h"
#define FLOW_SET_SIZE 4096u
static uint32_t g_flow_set[16][FLOW_SET_SIZE] __rte_cache_aligned; static uint32_t g_flow_seen_epoch[16][FLOW_SET_SIZE] __rte_cache_aligned;
void track_flow(unsigned wi,uint32_t sig){ const uint32_t mask=FLOW_SET_SIZE-1u; uint32_t idx=sig & mask; for(unsigned probe=0; probe<8u; ++probe){ if (g_flow_seen_epoch[wi][idx] != g_epoch){ g_flow_seen_epoch[wi][idx]=g_epoch; g_flow_set[wi][idx]=sig; g_flow_count[wi]++; return; } if (g_flow_set[wi][idx]==sig){ return; } idx=(idx+1u)&mask; } }
uint16_t pick_worker(uint32_t h){ return (uint16_t)g_reta[h & RETA_MASK]; }
struct dist_item { struct rte_mbuf *m; uint16_t wi; uint32_t flow_sig; };
int distA_main(void *arg){ (void)arg; puts("[Distributor-A] started (FAT: 8B, 56+3+5; XXH32/XXH64)"); struct rte_mbuf *rx[BURST]; struct dist_item *items[BURST]; while(!g_quit){ unsigned n=rte_ring_dequeue_burst(g_ingress_ring,(void**)rx,BURST,NULL); if(unlikely(n==0)){ rte_pause(); continue;} g_dist_rx+=n; unsigned w=0; for(unsigned i=0;i<n;i++){ if(likely(i+1<n)){ rte_prefetch0(rte_pktmbuf_mtod(rx[i+1], void*)); } rte_prefetch0(rte_pktmbuf_mtod(rx[i], void*)); struct dist_item *di=NULL; if(unlikely(rte_mempool_get(g_pipe_pool,(void**)&di)!=0 || di==NULL)){ rte_pktmbuf_free(rx[i]); g_dist_drop++; items[i]=NULL; continue; } const uint8_t *p=rte_pktmbuf_mtod(rx[i], const uint8_t*); const uint8_t *ip=p+14; const uint8_t *l4=ip+20; uint8_t tuple13[13]; memcpy(tuple13, ip+12,4); memcpy(tuple13+4, ip+16,4); tuple13[8]=ip[9]; tuple13[9]=l4[0]; tuple13[10]=l4[1]; tuple13[11]=l4[2]; tuple13[12]=l4[3]; uint64_t h64=xxh64(tuple13,sizeof(tuple13),XXH64_SEED); uint32_t h32=xxh32(tuple13,sizeof(tuple13),XXH32_SEED); uint64_t fp56=h64>>8; uint16_t wi; if(fat_lookup_tag(fp56,h64,&wi)){ g_fat_hits++; } else { uint32_t reta_idx=(h32>>24) & 0xFF; wi=pick_worker(reta_idx); fat_insert_tag(fp56,h64,wi); g_fat_misses++; } di->m=rx[i]; di->wi=wi; di->flow_sig=h32; items[w++]=di; } if(w){ unsigned pushed=rte_ring_enqueue_burst(g_dist_pipe,(void**)items,w,NULL); if(unlikely(pushed<w)){ for(unsigned i=pushed;i<w;i++){ if(items[i]){ rte_pktmbuf_free(items[i]->m); rte_mempool_put(g_pipe_pool, items[i]); g_dist_drop++; } } } } } return 0; }
int distB_main(void *arg){ (void)arg; puts("[Distributor-B] started"); struct dist_item *items[BURST]; struct rte_mbuf *wk_pkts[16][BURST]; uint16_t wk_cnt[16]; while(!g_quit){ unsigned n=rte_ring_dequeue_burst(g_dist_pipe,(void**)items,BURST,NULL); if(unlikely(n==0)){ rte_pause(); continue;} for(unsigned wi=0; wi<NB_WORKERS; wi++){ wk_cnt[wi]=0; } for(unsigned i=0;i<n;i++){ rte_prefetch0(items[i]); if(likely(i+1<n)) rte_prefetch0(items[i+1]); } for(unsigned i=0;i<n;i++){ struct dist_item *di=items[i]; if(unlikely(!di)) continue; unsigned wi=di->wi; struct rte_mbuf *m=di->m; uint32_t sig=di->flow_sig; if(unlikely(wi>=NB_WORKERS)){ rte_pktmbuf_free(m); rte_mempool_put(g_pipe_pool,di); g_dist_drop++; continue; } unsigned pos=wk_cnt[wi]; if(pos<BURST){ wk_pkts[wi][pos]=m; wk_cnt[wi]=(uint16_t)(pos+1); track_flow(wi,sig);} else { unsigned sent=rte_ring_enqueue_burst(g_worker_rings[wi],(void**)wk_pkts[wi],pos,NULL); g_dist_tx+=sent; for(unsigned j=sent;j<pos;j++){ rte_pktmbuf_free(wk_pkts[wi][j]); g_worker_drop[wi]++; g_dist_drop++; } wk_cnt[wi]=0; wk_pkts[wi][wk_cnt[wi]++]=m; } rte_mempool_put(g_pipe_pool, di);} for(unsigned wi=0; wi<NB_WORKERS; wi++){ unsigned cnt=wk_cnt[wi]; if(!cnt) continue; unsigned sent=rte_ring_enqueue_burst(g_worker_rings[wi],(void**)wk_pkts[wi],cnt,NULL); g_dist_tx+=sent; for(unsigned j=sent;j<cnt;j++){ rte_pktmbuf_free(wk_pkts[wi][j]); g_worker_drop[wi]++; g_dist_drop++; } wk_cnt[wi]=0; } } return 0; }
